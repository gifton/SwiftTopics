# 3.3 HDBSCAN Hierarchy

> **Building a dendrogram of clusters across all density levels.**

---

## The Concept

HDBSCAN's key insight: instead of picking **one** density threshold (Îµ), consider **all** thresholds simultaneously.

```
DBSCAN: "Find clusters at density level Îµ = 0.5"
        Single snapshot at one density.

HDBSCAN: "Find clusters at ALL density levels, then pick the stable ones"
         Movie of clusters forming and merging.
```

This creates a **hierarchy** (dendrogram) showing how clusters relate across densities:

```
Density (Î» = 1/distance):

    High Î»                   â—   â—   â—   â—   â—   â—   â—   â—
    (Dense)                   \ / \ /     \ / \ /
                               â—   â—       â—   â—
                                \ /         \ /
                                 â—           â—
                                  \         /
    Low Î»                          \       /
    (Sparse)                        â”€â”€â”€â—â”€â”€â”€
                                       â”‚
                              (Single cluster at Î»=0)

Reading top-to-bottom:
- At high density, many small clusters
- As density decreases, clusters merge
- At very low density, everything is one cluster
```

---

## Why It Matters

The hierarchy solves DBSCAN's variable-density problem:

```
Data with two different densities:

    Dense cluster          Sparse cluster
    â—â—â—â—â—â—â—â—â—â—â—           â—     â—
    â—â—â—â—â—â—â—â—â—â—â—â—             â—
    â—â—â—â—â—â—â—â—â—â—â—           â—     â—

DBSCAN at Îµ=0.2:
    [Dense cluster] âœ“      [Noise] âœ—

DBSCAN at Îµ=1.5:
    [Everything merged into one cluster] âœ—

HDBSCAN:
    Sees dense cluster BORN at Î»=5.0, DIES at Î»=0.5
    Sees sparse cluster BORN at Î»=0.7, DIES at Î»=0.3

    Both clusters are found and kept separate!
```

---

## The Mathematics

### Lambda Space (Density Space)

HDBSCAN works in **lambda space** where Î» = 1/distance:

```
Î» = 1/distance

distance â†’ Î»
   0.1   â†’ 10.0  (Very dense - close together)
   0.5   â†’ 2.0   (Moderately dense)
   1.0   â†’ 1.0
   2.0   â†’ 0.5   (Sparse - far apart)
   10.0  â†’ 0.1   (Very sparse)

Higher Î» = higher density = points closer together
```

### Cluster Lifecycle

Each cluster has a **birth** and **death** in lambda space:

```
                    Î» (density)
                        â†‘
    Î»_birth â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Cluster BORN
                        â”‚                     (First appears at this density)
                        â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                        â”‚  â–ˆ Cluster C  â–ˆâ–ˆ
                        â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    Î»_death â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Cluster DIES
                        â”‚                     (Merges with another cluster)
                        â†“

Lifespan of C = Î»_birth - Î»_death

Longer lifespan = more "stable" cluster
(Persists across many density levels)
```

### Stability Score

The stability of a cluster measures its persistence:

```
stability(C) = Î£ (Î»_death(p) - Î»_birth(C))  for all points p in C

Where:
  Î»_birth(C) = density level where cluster C first appeared
  Î»_death(p) = density level where point p left cluster C (or cluster died)

Intuition:
  - Each point contributes (how long it was in the cluster)
  - Large clusters with long lifespans have high stability
  - Small or fleeting clusters have low stability
```

---

## The Technique: Building the Hierarchy

### Step 1: Sort Edges by Distance

After computing mutual reachability distances (next guide), we have a weighted graph. Sort edges:

```
MST edges (sorted by weight/distance):

Edge      Weight (distance)    Î» = 1/weight
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
(A,B)          0.1              10.0
(C,D)          0.2               5.0
(B,E)          0.3               3.3
(D,F)          0.4               2.5
(E,G)          0.8               1.25
(A,C)          1.5               0.67
(F,H)          3.0               0.33
```

### Step 2: Process Edges in Order

Starting with each point in its own cluster, process edges:

```
Initial state (Î» = âˆ): Each point is its own cluster
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   A    B    C    D    E    F    G    H
   â—    â—    â—    â—    â—    â—    â—    â—
  [0]  [1]  [2]  [3]  [4]  [5]  [6]  [7]

Process edge (A,B) at Î» = 10.0:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   Aâ”€â”€â”€â”€B    C    D    E    F    G    H
    [8]     [2]  [3]  [4]  [5]  [6]  [7]

   Cluster 8 is BORN (merging 0 and 1)
   Birth level: Î» = 10.0

Process edge (C,D) at Î» = 5.0:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   Aâ”€â”€â”€â”€B    Câ”€â”€â”€â”€D    E    F    G    H
    [8]       [9]     [4]  [5]  [6]  [7]

   Cluster 9 is BORN

Process edge (B,E) at Î» = 3.3:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   Aâ”€â”€â”€â”€Bâ”€â”€â”€â”€E    Câ”€â”€â”€â”€D    F    G    H
       [10]         [9]    [5]  [6]  [7]

   Cluster 10 is BORN (merging 8 and 4)
   Cluster 8 DIES at Î» = 3.3

... Continue until all points are in one cluster ...

Final state (Î» â†’ 0):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                    [Root]
                   â•±      â•²
                  â•±        â•²
               [11]        [H]
              â•±    â•²
            [10]   [9]
           â•±  â•²   â•±  â•²
         [8] [E] [C] [D]
        â•±  â•²
       [A] [B]
```

### Step 3: Build the Dendrogram

The resulting tree (dendrogram) shows all cluster relationships:

```
                          Î» (density)
                             â”‚
    10.0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€ [8]: {A,B} born
                             â”‚
     5.0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€ [9]: {C,D} born
                             â”‚
     3.3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€ [8] dies â†’ [10]: {A,B,E}
                             â”‚
     2.5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€ [9] dies â†’ [11]: {C,D,F}
                             â”‚
     1.25 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€ [10] and [11] merge â†’ [12]
                             â”‚
     0.67 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€ [12] absorbs G
                             â”‚
     0.33 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€ [13]: All points
                             â”‚
     0.0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€

Dendrogram view (rotated):

                [13: Root]
               â•±          â•²
           [12]            [H]
          â•±    â•²
      [10]      [11]
     â•±  |       |  â•²
  [8]  [E]    [9]  [F]
 â•±  â•²        â•±  â•²
[A] [B]    [C] [D]
```

---

## In SwiftTopics

### The Cluster Hierarchy

```swift
// ğŸ“ See: Sources/SwiftTopics/Clustering/HDBSCAN/ClusterHierarchyBuilder.swift

/// A node in the cluster hierarchy.
public struct ClusterHierarchyNode: Sendable, Identifiable {
    /// Unique identifier (points: 0..<n, internal: n..<2n-1)
    public let id: Int

    /// Parent node ID (nil for root)
    public let parent: Int?

    /// Child node IDs (empty for leaves/points)
    public let children: [Int]

    /// Birth level (distance at which cluster formed)
    public let birthLevel: Float

    /// Death level (distance at which cluster merged into parent)
    public let deathLevel: Float

    /// Number of points in this cluster
    public let size: Int

    /// Stability score
    public let stability: Float
}
```

### Building the Hierarchy

```swift
// ğŸ“ See: Sources/SwiftTopics/Clustering/HDBSCAN/ClusterHierarchyBuilder.swift

public struct ClusterHierarchyBuilder: Sendable {
    /// Minimum cluster size for stability computation.
    public let minClusterSize: Int

    /// Builds the cluster hierarchy from an MST.
    public func build(
        from mst: MinimumSpanningTree,
        allowSingleCluster: Bool = false
    ) -> ClusterHierarchy {
        let sortedEdges = mst.sortedEdges  // Ascending by weight

        var clusterState = HierarchyBuildState(pointCount: mst.pointCount)

        // Process edges in order (low distance = high density first)
        for edge in sortedEdges {
            clusterState.mergePoints(
                edge.source,
                edge.target,
                atDistance: edge.weight
            )
        }

        return clusterState.finalize(
            minClusterSize: minClusterSize,
            allowSingleCluster: allowSingleCluster
        )
    }
}
```

### The Stability Calculation

```swift
// ğŸ“ See: Sources/SwiftTopics/Clustering/HDBSCAN/ClusterHierarchyBuilder.swift

/// Computes stability for a single node.
///
/// stability(C) = Î£ (Î»_death(p) - Î»_birth(C)) for points p in C
///
/// Where Î» = 1/distance.
private func computeNodeStability(
    node: ClusterHierarchyNode,
    nodes: [ClusterHierarchyNode],
    nodeByID: [Int: Int]
) -> Float {
    let birthDistance = node.birthLevel
    let deathDistance = node.deathLevel

    // Convert to lambda (density) space
    // Î» = 1/distance
    let lambdaBirth = birthDistance > Float.ulpOfOne
        ? 1.0 / birthDistance
        : Float.infinity
    let lambdaDeath = deathDistance > Float.ulpOfOne && deathDistance < Float.infinity
        ? 1.0 / deathDistance
        : 0

    // Collect all leaf descendants
    let leaves = collectLeafDescendants(nodeID: node.id, ...)

    var stability: Float = 0
    for (_, leafDeathDistance) in leaves {
        let leafLambdaDeath = leafDeathDistance > Float.ulpOfOne
            ? 1.0 / leafDeathDistance
            : 0

        // Contribution = (Î»_death - Î»_birth) for this point's membership
        let contribution = max(0, lambdaBirth - max(leafLambdaDeath, lambdaDeath))
        stability += contribution
    }

    return stability
}
```

---

## Visualizing Stability

```
Two potential clusterings from the same hierarchy:

Option A: Select parent cluster [10]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           [10]
          â•±    â•²
       [8]      [9]
       (died)   (died)

   Stability([10]) = 0.8
   (Large cluster, long lifespan)

Option B: Select children [8] and [9]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        [8]      [9]
       (kept)   (kept)

   Stability([8]) + Stability([9]) = 0.3 + 0.4 = 0.7
   (Two smaller clusters, shorter lifespans)

If Stability([10]) > Stability([8]) + Stability([9]):
   â†’ Select the parent [10]

If Stability([10]) < Stability([8]) + Stability([9]):
   â†’ Select the children [8] and [9]
```

---

## The Condensed Tree

HDBSCAN filters out small clusters to create a **condensed tree**:

```swift
// ğŸ“ See: Sources/SwiftTopics/Clustering/HDBSCAN/ClusterHierarchyBuilder.swift

/// A condensed view of the cluster tree for cluster extraction.
///
/// The condensed tree removes clusters smaller than minClusterSize,
/// keeping only significant clusters for selection.
public struct CondensedTree: Sendable {
    /// Nodes in the condensed tree.
    public let nodes: [CondensedTreeNode]

    /// Root node ID.
    public let rootID: Int
}

// Before condensing (minClusterSize = 5):
//
//     [Root: 100 pts]
//    â•±      |      â•²
// [40 pts] [30] [30]      [Noise: various small]
//           |      â•²
//         [15]     [15]
//           |
//          [8]
//
// After condensing:
//
//     [Root: 100 pts]
//    â•±      |      â•²
// [40 pts] [30] [30]
//           |      â•²
//         [15]     [15]
//           |
//          [8]
//
// Clusters < 5 points removed from consideration
```

---

## Why Hierarchy Beats Flat Clustering

### Advantage 1: Multi-Resolution

```
Your journal might have:
  - Broad topic: "Health" (200 entries)
  - Sub-topic: "Fitness" (120 entries)
  - Sub-sub-topic: "Running" (50 entries)

Hierarchy captures all levels:

        [Health: 200]
       â•±      |      â•²
  [Fitness] [Mental] [Diet]
      â”‚
  [Running]
      â”‚
  [Marathon Training]

You can extract at any granularity!
```

### Advantage 2: Stable Selection

```
Flat clustering is binary: in or out.
Hierarchical clustering measures confidence.

High stability cluster:
  - Born early (high density)
  - Died late (persisted long)
  - Many points
  â†’ Confident this is a real topic

Low stability cluster:
  - Born late
  - Died quickly
  - Few points
  â†’ Might just be noise
```

### Advantage 3: Outlier Quality

```
Outliers have context in the hierarchy:

Point X is noise because:
  - It never joined a stable cluster
  - It briefly joined [ClusterY] but [ClusterY] was unstable
  - It's far from all dense regions

You can inspect WHY something is an outlier.
```

---

## âš ï¸ Common Pitfalls

### Pitfall 1: Ignoring minClusterSize

```swift
// âš ï¸ TOO SMALL: Every noise point becomes a "cluster"
let config = HDBSCANConfiguration(minClusterSize: 2)
// Result: 50 "clusters", most with 2-3 points

// âš ï¸ TOO LARGE: Misses real small topics
let config = HDBSCANConfiguration(minClusterSize: 50)
// Result: 3 clusters, everything else is noise

// âœ… BALANCED: Tune based on expected topic size
let config = HDBSCANConfiguration(minClusterSize: 5)
// 5-10 is usually a good starting point
```

### Pitfall 2: Expecting Perfect Nesting

```swift
// âš ï¸ WRONG: Assuming hierarchy = topic taxonomy

// The hierarchy shows DENSITY relationships, not SEMANTIC relationships.
// "Running" might merge with "Diet" before "Fitness" if their embeddings
// happen to have overlapping dense regions.

// Don't interpret hierarchy as a topic taxonomy!
```

### Pitfall 3: Forgetting About Lambda Space

```swift
// âš ï¸ CONFUSING: Thinking in distance when stability uses Î»

// Low distance = high Î» (dense, born early)
// High distance = low Î» (sparse, born late)

// A cluster "born" at distance 0.1 is born at Î» = 10 (dense)
// A cluster "born" at distance 2.0 is born at Î» = 0.5 (sparse)
```

---

## Key Takeaways

1. **Hierarchy captures all densities**: No need to pick one Îµ value.

2. **Lambda space measures density**: Î» = 1/distance; higher Î» = denser.

3. **Clusters have lifecycles**: Birth (appear), death (merge), lifespan (stability).

4. **Stability scores matter**: Longer-lived clusters are more reliable.

5. **Condensed tree filters noise**: Small clusters removed from consideration.

6. **Multi-resolution output**: Can extract at different granularities.

---

## ğŸ’¡ Key Insight

The hierarchy is HDBSCAN's secret weapon. Instead of asking "what clusters exist at density X?", it asks "what clusters exist **across all densities** and which are most stable?"

```
DBSCAN: Snapshot photography (one moment in time)
HDBSCAN: Time-lapse video (all moments, find the stable patterns)

Stable patterns = real clusters
Fleeting patterns = noise
```

---

## Next Up

We've seen the big picture. Now let's understand the key enabler: **mutual reachability distance**, which makes the hierarchy handle varying densities.

**[â†’ 3.4 Mutual Reachability](./04-Mutual-Reachability.md)**

---

*Guide 3.3 of 3.5 â€¢ Chapter 3: Density-Based Clustering*
