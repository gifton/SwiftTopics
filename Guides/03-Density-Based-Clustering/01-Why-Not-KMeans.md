# 3.1 Why Not K-Means

> **The most popular clustering algorithm‚Äîand why it fails for topic modeling.**

---

## The Concept

**K-Means** is the most widely known clustering algorithm. It's simple, fast, and works well for many problems. So why doesn't SwiftTopics use it?

K-Means has four fundamental assumptions that clash with topic modeling:

1. **You know K beforehand** ‚Äî The number of clusters must be specified
2. **Clusters are spherical** ‚Äî Points group around centroids in balls
3. **Clusters are similar size** ‚Äî Equal variance assumed
4. **Every point belongs to a cluster** ‚Äî No concept of noise/outliers

Topic modeling violates all four.

---

## Why It Matters

Understanding K-Means' limitations clarifies what HDBSCAN provides and when K-Means might still be appropriate.

### The K Problem

```
Real journals have unknown topic counts:

Journal A: 3 topics (work, fitness, family)
Journal B: 12 topics (varied interests)
Journal C: 47 topics (20 years of entries)

K-Means requires:
  KMeans(k: ???)  // What value?

Options:
  1. Guess ‚Üí Wrong clusters
  2. Try many K values ‚Üí Slow, still guessing
  3. Use heuristics (elbow method) ‚Üí Often misleading
```

### The Shape Problem

```
Topics in embedding space aren't spherical:

K-Means assumes:         Topics actually look like:

     ‚óã ‚óã ‚óã                    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
    ‚óã ‚óè ‚óã ‚óã                  ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
     ‚óã ‚óã ‚óã                    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
                               ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
  (Spherical around             (Elongated manifold
   centroid ‚óè)                   following semantic
                                 similarity)

The "anxiety" topic might curve through embedding space,
not form a neat ball around a center.
```

### The Size Problem

```
Real topics have varying sizes:

[==========] "Work" - 500 entries
[====] "Family" - 200 entries
[==] "Travel" - 100 entries
[=] "Astronomy" - 15 entries

K-Means tends toward equal-sized clusters:

Original:                K-Means result:
[==========]            [======]  ‚Üê Split Work
[====]                  [====]
[==]                    [====]  ‚Üê Merged Travel+Astronomy
[=]                     [====]  ‚Üê Partial Work
```

### The Noise Problem

```
Some entries don't belong anywhere:

"Today I discovered that my neighbor
 is actually three raccoons in a trench coat."

This is:
- Not fitness
- Not work
- Not family
- Not any typical topic

K-Means: Forces into nearest cluster (wrong!)
HDBSCAN: Marks as noise (-1)
```

---

## The Mathematics

### K-Means Objective

K-Means minimizes the **within-cluster sum of squares** (WCSS):

```
WCSS = Œ£‚Çñ Œ£·µ¢‚ààC‚Çñ ||x·µ¢ - Œº‚Çñ||¬≤

Where:
  k = cluster index (1 to K)
  C‚Çñ = set of points in cluster k
  x·µ¢ = point i
  Œº‚Çñ = centroid of cluster k

This is minimized when points are close to their cluster centroid.
```

### The Algorithm

```
K-Means Algorithm:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

1. Choose K random points as initial centroids

2. Repeat until convergence:
   a. Assign each point to nearest centroid
   b. Recompute centroids as mean of assigned points

3. Return assignments and final centroids

Complexity: O(n √ó K √ó d √ó iterations)
Where: n = points, K = clusters, d = dimensions
```

### Why Spherical?

The distance to centroid metric creates **Voronoi regions**:

```
K=3 centroids (marked ‚óè):

        ‚îÇ       ‚ï±
        ‚îÇ      ‚ï±
     C0 ‚îÇ C1  ‚ï±
        ‚îÇ    ‚ï±  C2
        ‚îÇ   ‚ï±
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        ‚îÇ ‚ï±
        ‚îÇ‚ï±
       ‚ï±‚îÇ
      ‚ï± ‚îÇ

Each region contains points closest to that centroid.
Boundaries are straight lines (hyperplanes in high-D).

This creates convex, approximately spherical regions.
Non-spherical clusters get split by these boundaries.
```

---

## The Technique: Comparing Behaviors

### Scenario 1: Natural Spherical Clusters

```
Data with 3 well-separated spherical clusters:

      ‚óè‚óè‚óè                    ‚óè‚óè‚óè
     ‚óè‚óè‚óè‚óè‚óè                  ‚óè‚óè‚óè‚óè‚óè
      ‚óè‚óè‚óè        ‚Üí          ‚óè‚óè‚óè
                            (Cluster 0)
          ‚óè‚óè‚óè‚óè‚óè                  ‚óè‚óè‚óè‚óè‚óè
         ‚óè‚óè‚óè‚óè‚óè‚óè‚óè               ‚óè‚óè‚óè‚óè‚óè‚óè‚óè
          ‚óè‚óè‚óè‚óè‚óè                  ‚óè‚óè‚óè‚óè‚óè
                               (Cluster 1)
                  ‚óè‚óè‚óè‚óè                  ‚óè‚óè‚óè‚óè
                 ‚óè‚óè‚óè‚óè‚óè‚óè               ‚óè‚óè‚óè‚óè‚óè‚óè
                  ‚óè‚óè‚óè‚óè                  ‚óè‚óè‚óè‚óè
                                      (Cluster 2)

K-Means: Works well! ‚úì
HDBSCAN: Also works well ‚úì
```

### Scenario 2: Elongated Cluster

```
Data with one elongated cluster:

    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè

K-Means (K=1):
    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óã‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
                   (Single centroid in middle)
    Works! ‚úì

K-Means (K=2):
    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óã‚óè‚óè‚óè‚óè‚îÇ‚óè‚óè‚óè‚óè‚óè‚óè‚óã‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
    (Cluster 0)    ‚îÇ     (Cluster 1)
    Splits the natural cluster ‚úó

K-Means (K=3):
    ‚óè‚óè‚óè‚óè‚óè‚óè‚óã‚óè‚óè‚óè‚îÇ‚óè‚óè‚óè‚óè‚óã‚óè‚óè‚óè‚óè‚îÇ‚óè‚óè‚óè‚óè‚óã‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
    (C0)      ‚îÇ   (C1)  ‚îÇ       (C2)
    Further fragmentation ‚úó

HDBSCAN:
    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
    (All one cluster - follows density) ‚úì
```

### Scenario 3: Varying Density

```
Dense region + sparse region:

    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè              ‚óè        ‚óè
    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè           ‚óè    ‚óè  ‚óè
    (200 points)               (10 points)

K-Means (K=2):
    [=====‚óè=====]           [    ‚óè    ]
    Centroid here           Centroid here
    Works for dense,        Sparse points forced
    but...                  into a "cluster"

What if sparse points are actually 3 different topics?
K-Means merges them.

HDBSCAN:
    [Dense cluster]         ‚úó ‚úó ‚úó ‚úó ‚úó
    Natural grouping        Marked as noise

Or with lower minClusterSize:
    [Dense cluster]         [Small cluster 1]
                           [Small cluster 2]
                           [Small cluster 3]
```

### Scenario 4: Unknown K

```
You have 1000 journal entries. How many topics?

Approach 1: Elbow Method
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Run K-Means for K = 2, 3, 4, ..., 30
Plot WCSS vs K
Look for "elbow" where improvement slows

    WCSS
      ‚îÇ‚ï≤
      ‚îÇ ‚ï≤
      ‚îÇ  ‚ï≤
      ‚îÇ   ‚ï≤__________     ‚Üê Elbow around K=5?
      ‚îÇ              ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ K
         5   10  15  20

Problem: Often ambiguous. No clear elbow.

Approach 2: Silhouette Score
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Measure cluster quality for each K
Pick K with highest silhouette score

Problem: Computationally expensive. Still heuristic.

HDBSCAN Approach:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
let result = try await hdbscan.fit(embeddings)
print(result.clusterCount)  // Just get the answer

No iteration. Clusters emerge from data structure.
```

---

## In SwiftTopics

SwiftTopics doesn't use K-Means for topic discovery. Instead:

```swift
// üìç See: Sources/SwiftTopics/Clustering/HDBSCAN/HDBSCAN.swift

public actor HDBSCANEngine: ClusteringEngine {
    /// Clusters the given embeddings using HDBSCAN.
    ///
    /// - Parameter embeddings: The embeddings to cluster.
    /// - Returns: Cluster assignments for each embedding.
    public func fit(_ embeddings: [Embedding]) async throws -> ClusterAssignment {
        // 1. Compute core distances
        // 2. Build mutual reachability graph
        // 3. Construct minimum spanning tree
        // 4. Build cluster hierarchy
        // 5. Extract stable clusters
        //
        // No K required. Cluster count emerges.
    }
}
```

### The ClusterAssignment Result

```swift
// üìç See: Sources/SwiftTopics/Clustering/ClusterAssignment.swift

public struct ClusterAssignment: Sendable {
    /// Cluster labels for each point. -1 indicates noise.
    public let labels: [Int]

    /// Membership probability for each point.
    public let probabilities: [Float]

    /// Outlier score for each point (higher = more outlier-like).
    public let outlierScores: [Float]

    /// Number of clusters found (excluding noise).
    public let clusterCount: Int
}

// Usage:
let result = try await hdbscan.fit(embeddings)

print("Found \(result.clusterCount) topics")

for i in 0..<embeddings.count {
    let label = result.label(for: i)
    if label == -1 {
        print("Document \(i) is an outlier")
    } else {
        print("Document \(i) belongs to topic \(label)")
    }
}
```

---

## When K-Means IS Appropriate

K-Means isn't wrong‚Äîit's just wrong for **topic discovery**. Use K-Means when:

### 1. K Is Known

```
Scenario: Categorizing products into predetermined departments

Departments = ["Electronics", "Clothing", "Home", "Food", "Sports"]
K = 5 (known beforehand)

K-Means works well here.
```

### 2. Speed Is Critical

```
K-Means: O(n √ó K √ó d √ó iterations)  ‚Äî Very fast
HDBSCAN: O(n¬≤ log n) or O(n √ó k) with GPU ‚Äî Slower

For millions of points where K is acceptable,
K-Means is often the practical choice.
```

### 3. Downstream Requires Fixed K

```
Scenario: Training a classifier with 10 categories

You need exactly 10 cluster centroids as class prototypes.
K-Means guarantees exactly K outputs.
```

### 4. Data Is Well-Behaved

```
When your data actually has:
- Similar-sized clusters
- Roughly spherical distributions
- No significant outliers
- Known number of groups

K-Means is simpler and faster.
```

---

## ‚ö†Ô∏è Common Pitfalls

### Pitfall 1: Using K-Means for Exploration

```swift
// ‚ùå WRONG: Using K-Means to discover topics
let kmeans = KMeans(k: 10)  // Why 10? Arbitrary!
let topics = kmeans.fit(embeddings)

// You might have 7 natural topics, but you forced 10.
// Or you might have 30 natural topics, but you limited to 10.
```

### Pitfall 2: Ignoring the Elbow

```swift
// ‚ö†Ô∏è MISLEADING: Blind trust in elbow method
for k in 2...30 {
    let wcss = kmeans(embeddings, k: k).wcss
    print("K=\(k): WCSS=\(wcss)")
}
// Looking for an "elbow" that may not exist
```

### Pitfall 3: Assuming K-Means Outliers Are Valid

```swift
// ‚ùå WRONG: Treating all K-Means assignments as meaningful
let labels = kmeans.fit(embeddings).labels
// Every point has a label. No outlier detection.
// That weird entry about raccoons? Forced into "Family" topic.
```

### Pitfall 4: K-Means on High-Dimensional Data

```swift
// ‚ö†Ô∏è PROBLEMATIC: K-Means on 768D embeddings
let labels = kmeans.fit(rawEmbeddings)  // 768 dimensions!

// Distance concentration makes all points equidistant.
// Centroids become meaningless.
// Always reduce dimensions first!

// ‚úÖ CORRECT: Reduce first
let reduced = try await pca.transform(rawEmbeddings)  // 768D ‚Üí 15D
let labels = clustering.fit(reduced)
```

---

## Key Takeaways

1. **K-Means requires K**: You must specify cluster count upfront.

2. **K-Means assumes spherical clusters**: Real topics are irregularly shaped.

3. **K-Means forces assignment**: Every point belongs somewhere‚Äîno outlier concept.

4. **K-Means prefers equal sizes**: Biased toward similar-sized clusters.

5. **K-Means is still useful**: When K is known, speed matters, and data is well-behaved.

6. **HDBSCAN addresses all these issues**: Discovers K, handles shapes, marks outliers.

---

## üí° Key Insight

K-Means is a **partitioning** algorithm‚Äîit divides space into K regions. HDBSCAN is a **density** algorithm‚Äîit finds regions where points are densely packed.

```
K-Means asks: "How do I best split this space into K pieces?"
HDBSCAN asks: "Where are the dense regions in this space?"

For topic modeling, the second question is the right one.
Your journal entries naturally cluster around themes.
You don't need to decide how many themes beforehand.
```

---

## Next Up

Now that we understand why density-based clustering, let's learn the foundation: **DBSCAN**.

**[‚Üí 3.2 DBSCAN Foundation](./02-DBSCAN-Foundation.md)**

---

*Guide 3.1 of 3.5 ‚Ä¢ Chapter 3: Density-Based Clustering*
