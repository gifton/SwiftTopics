# 3.2 DBSCAN Foundation

> **The original density-based clustering algorithmâ€”and why we needed HDBSCAN.**

---

## The Concept

**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) was a breakthrough: the first practical clustering algorithm that didn't require K and could mark outliers.

The core idea is elegant:

> **Clusters are dense regions separated by sparse regions.**

```
What DBSCAN sees:

    Dense region 1        Sparse gap        Dense region 2
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  â—â—â—â—â—â—â—â—â—â—    â”‚   â”‚          â”‚   â”‚    â—â—â—â—â—â—â—â—â—â—  â”‚
    â”‚ â—â—â—â—â—â—â—â—â—â—â—â—   â”‚   â”‚  â—    â—  â”‚   â”‚   â—â—â—â—â—â—â—â—â—â—â—â— â”‚
    â”‚  â—â—â—â—â—â—â—â—â—â—    â”‚   â”‚          â”‚   â”‚    â—â—â—â—â—â—â—â—â—â—  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         Cluster 0           Noise           Cluster 1
```

---

## Why It Matters

DBSCAN introduced three concepts that HDBSCAN builds upon:

1. **Core Points**: Points with enough neighbors (density indicators)
2. **Border Points**: Points on the edge of clusters
3. **Noise Points**: Points in sparse regions (outliers)

Understanding these is essential for understanding HDBSCAN.

---

## The Mathematics

### Two Parameters

DBSCAN uses two parameters:

```
Îµ (epsilon): Maximum distance for a point to be considered a neighbor
minPts:      Minimum neighbors to be considered a core point

These define what "dense" means:
- A point is in a dense region if it has â‰¥ minPts neighbors within distance Îµ
```

### Point Classification

```
For each point p:

Count neighbors within Îµ:
  N_Îµ(p) = { q : distance(p, q) â‰¤ Îµ }

If |N_Îµ(p)| â‰¥ minPts:
  p is a CORE POINT (in a dense region)

If |N_Îµ(p)| < minPts but p is within Îµ of a core point:
  p is a BORDER POINT (on the edge of a cluster)

If |N_Îµ(p)| < minPts and p is not near any core point:
  p is a NOISE POINT (outlier)
```

### Visual Classification

```
Îµ = the radius of circles
minPts = 4

     â—â”€â”€â”€â—â”€â”€â”€â—          Legend:
    â•±â”‚â•² â•± â•² â•±â”‚â•²         â— Core point (â‰¥4 neighbors in Îµ)
   â— â”‚ â—   â— â”‚ â—        â—‹ Border point (<4 but near core)
    â•²â”‚â•±     â•²â”‚â•±         âœ— Noise point (sparse, isolated)
     â—â”€â”€â”€â”€â”€â”€â”€â—
              â•²
               â—‹        Border: Only 2 neighbors,
                          but within Îµ of core â—
       âœ—                Noise: Isolated, no core nearby

Each core point can "reach" other core points through chains.
All reachable core points form one cluster.
```

---

## The Technique: DBSCAN Algorithm

### Step by Step

```
DBSCAN Algorithm:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Input: Points, Îµ, minPts
Output: Cluster labels (-1 for noise)

1. Mark all points as UNVISITED

2. For each UNVISITED point p:
   a. Mark p as VISITED
   b. Find all neighbors within Îµ: N = neighbors(p, Îµ)

   c. If |N| < minPts:
      - Mark p as NOISE (may change later)

   d. If |N| â‰¥ minPts:
      - Create new cluster C
      - Add p to C
      - For each point q in N:
        * If q is UNVISITED:
          - Mark q as VISITED
          - Find neighbors of q: N' = neighbors(q, Îµ)
          - If |N'| â‰¥ minPts: add N' to N (expand cluster)
        * If q is not in any cluster: add q to C

3. Return cluster assignments

Complexity: O(nÂ²) naive, O(n log n) with spatial index
```

### Walkthrough Example

```
Points: A, B, C, D, E, F, G, H
Îµ = 1.0, minPts = 3

Distance matrix (simplified):
        A     B     C     D     E     F     G     H
    A   0    0.5   0.8   1.5   2.0   2.5   3.0   5.0
    B  0.5    0    0.3   1.0   1.5   2.0   2.5   4.5
    C  0.8   0.3    0    0.7   1.2   1.7   2.2   4.2
    D  1.5   1.0   0.7    0    0.5   1.0   1.5   3.7
    E  2.0   1.5   1.2   0.5    0    0.5   1.0   3.2
    F  2.5   2.0   1.7   1.0   0.5    0    0.5   2.7
    G  3.0   2.5   2.2   1.5   1.0   0.5    0    2.2
    H  5.0   4.5   4.2   3.7   3.2   2.7   2.2    0

Step 1: Process A
  Neighbors(A, Îµ=1.0) = {B, C}  (distances 0.5, 0.8)
  |neighbors| = 2 < minPts(3)
  Mark A as NOISE (for now)

Step 2: Process B
  Neighbors(B, Îµ=1.0) = {A, C, D}
  |neighbors| = 3 â‰¥ minPts(3)
  B is CORE POINT â†’ Start Cluster 0
  Add B to Cluster 0
  Expand: check A, C, D

  A: neighbors = {B, C}, |N|=2 < 3, A is border, add to Cluster 0
  C: neighbors = {A, B, D}, |N|=3 â‰¥ 3, C is core, add neighbors to search
  D: neighbors = {B, C, E, F}, |N|=4 â‰¥ 3, D is core, add neighbors

  Continue expanding...

Final result:
  Cluster 0: {A, B, C, D, E, F, G}  (connected dense region)
  Noise: {H}  (isolated point)
```

---

## The Epsilon Problem

DBSCAN has a critical weakness: **Îµ must be chosen carefully**.

### Too Small Îµ

```
Îµ too small:

    Data:                     DBSCAN result (Îµ=0.3):

       â—â—â—â—â—â—                    â—‹ â—‹ â—‹ â—‹ â—‹ â—‹
      â—â—â—â—â—â—â—â—                  â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹
       â—â—â—â—â—â—                    â—‹ â—‹ â—‹ â—‹ â—‹ â—‹

    Natural cluster            All points become noise!
                              (No point has enough neighbors
                               within the tiny radius)
```

### Too Large Îµ

```
Îµ too large:

    Data:                     DBSCAN result (Îµ=5.0):

    â—â—â—â—â—â—     â—â—â—â—â—â—          â—â—â—â—â—â— â”€â”€â”€ â—â—â—â—â—â—
    (Cluster A) (Cluster B)    (All merged into one!)

    Two separate clusters      Îµ so large that they connect
```

### The "Right" Îµ

```
The Goldilocks problem:

Îµ = 0.3  â†’  All noise (too small)
Îµ = 0.5  â†’  Cluster 1 found, Cluster 2 becomes noise
Îµ = 0.8  â†’  Both clusters found! âœ“
Îµ = 1.2  â†’  Clusters merge (too large)
Îµ = 2.0  â†’  Everything is one cluster

The "right" Îµ depends on data density.
Different regions may need different Îµ values!
```

---

## The Variable Density Problem

This is DBSCAN's fatal flaw for topic modeling:

```
Real data has VARYING DENSITY:

    Dense cluster:            Sparse cluster:

    â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—          â—     â—     â—
    â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
    â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—           â—       â—
    (Points 0.1 apart)        (Points 1.0 apart)

With Îµ = 0.2:
  Dense cluster: Found! âœ“
  Sparse cluster: All noise âœ—

With Îµ = 1.5:
  Dense cluster: Found âœ“
  Sparse cluster: Found âœ“
  BUT: Dense cluster expands to connect to noise!

NO SINGLE Îµ WORKS FOR BOTH.
```

This is why HDBSCAN was invented.

---

## In SwiftTopics

SwiftTopics doesn't use vanilla DBSCAN. But understanding DBSCAN helps understand HDBSCAN's core distance concept:

```swift
// ğŸ“ See: Sources/SwiftTopics/Clustering/HDBSCAN/CoreDistance.swift

/// The core distance of a point is the distance to its k-th nearest neighbor,
/// which serves as a measure of local density. Points in dense regions have
/// small core distances; points in sparse regions have large core distances.

// DBSCAN: Fixed Îµ for all points
// HDBSCAN: Adaptive "Îµ" per point (core distance)

// DBSCAN check:
let isCore = neighbors.count >= minPts  // within fixed Îµ

// HDBSCAN approach:
let coreDistance = distanceToKthNeighbor(k: minSamples)
// Each point has its own "Îµ" based on local density
```

### The Core Distance Evolution

```swift
// ğŸ“ Conceptual evolution from DBSCAN to HDBSCAN:

// DBSCAN:
struct DBSCANPoint {
    let isCore: Bool  // Binary: has â‰¥ minPts neighbors within Îµ
}

// HDBSCAN:
struct HDBSCANPoint {
    let coreDistance: Float  // Continuous: distance to minPts-th neighbor
    // Density = 1 / coreDistance
    // Small coreDistance = dense region
    // Large coreDistance = sparse region
}
```

---

## Visualizing Core vs Border vs Noise

```
Example: 15 points with Îµ = 1.0, minPts = 4

     A(â—)â”€â”€â”€ B(â—)â”€â”€â”€ C(â—)
      â”‚â•²     â”‚      â•±â”‚
      â”‚ â•²    â”‚     â•± â”‚
      â”‚  â•²   â”‚    â•±  â”‚
     D(â—)â”€â”€ E(â—)â”€â”€ F(â—)
             â”‚
             â”‚
            G(â—‹)       H(âœ—)



Point  Neighbors (within Îµ)    Count   Classification
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  A     {B, D, E}               3      Border? (< 4) but wait...
  B     {A, C, E}               3      Border? checking...
  C     {B, E, F}               3      Border?
  D     {A, E}                  2      Border
  E     {A, B, C, D, F, G}      6      CORE âœ“
  F     {C, E}                  2      Border
  G     {E}                     1      Border (near core E)
  H     {}                      0      NOISE âœ—

Hmm, only E is core. Let's recalculate with minPts = 3:

  A     {B, D, E}               3      CORE âœ“
  B     {A, C, E}               3      CORE âœ“
  C     {B, E, F}               3      CORE âœ“
  D     {A, E}                  2      Border (near core A)
  E     {A, B, C, D, F, G}      6      CORE âœ“
  F     {C, E}                  2      Border (near core C)
  G     {E}                     1      Border (near core E)
  H     {}                      0      NOISE âœ—

Now we have one cluster containing A,B,C,D,E,F,G and one noise point H.
```

---

## âš ï¸ Common Pitfalls

### Pitfall 1: Not Scaling Data

```swift
// âŒ WRONG: DBSCAN on unscaled features
// Feature 1: salary (10,000 - 1,000,000)
// Feature 2: age (18 - 80)

let dbscan = DBSCAN(epsilon: 100, minPts: 5)
// Îµ = 100 is huge for age but tiny for salary!

// âœ… CORRECT: Normalize or standardize first
let normalized = standardize(data)
let dbscan = DBSCAN(epsilon: 0.5, minPts: 5)
```

### Pitfall 2: Using Distance Instead of Density Intuition

```swift
// âš ï¸ MISLEADING: Thinking of Îµ as "closeness"

// Wrong mental model:
// "Points within Îµ are 'close enough' to cluster"

// Better mental model:
// "Îµ defines what 'dense' means for this dataset"
// Core points are density indicators, not distance indicators.
```

### Pitfall 3: Expecting K Clusters

```swift
// âŒ WRONG: Expecting DBSCAN to find exactly K clusters
let result = dbscan.fit(data)
assert(result.clusterCount == 5)  // May not be 5!

// DBSCAN finds what's there, not what you want.
// If data has 3 dense regions, you get 3 clusters.
// If data is uniform, you may get 1 cluster or all noise.
```

### Pitfall 4: Ignoring Noise Points

```swift
// âš ï¸ MISTAKE: Treating noise as "bad" results
let noiseCount = result.labels.filter { $0 == -1 }.count
print("âš ï¸ \(noiseCount) points failed to cluster!")  // Wrong framing

// Better framing:
print("â„¹ï¸ \(noiseCount) points identified as outliers")
// Noise detection is a feature, not a bug!
```

---

## Key Takeaways

1. **DBSCAN uses density**: Clusters are dense regions; gaps are sparse regions.

2. **Three point types**: Core (dense), border (edge), noise (sparse).

3. **Two parameters**: Îµ (neighborhood radius) and minPts (density threshold).

4. **No K required**: Cluster count emerges from data.

5. **Handles outliers**: Noise points are explicitly identified.

6. **Fatal flaw**: Single Îµ can't handle varying-density data.

---

## ğŸ’¡ Key Insight

DBSCAN's genius is the density perspective. Its weakness is the global Îµ assumption.

```
DBSCAN's question: "Is this point in a dense region?"
                   (Using fixed definition of "dense")

HDBSCAN's question: "Is this point in a RELATIVELY dense region?"
                    (Compared to its local neighborhood)

HDBSCAN makes Îµ adaptive through "core distance":
- Dense regions: small effective Îµ
- Sparse regions: large effective Îµ

This is the mutual reachability transform we'll learn next.
```

---

## Next Up

Now we understand the foundation. Let's see how HDBSCAN extends DBSCAN to handle varying densities:

**[â†’ 3.3 HDBSCAN Hierarchy](./03-HDBSCAN-Hierarchy.md)**

---

*Guide 3.2 of 3.5 â€¢ Chapter 3: Density-Based Clustering*
